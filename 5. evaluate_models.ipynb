{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "from delong_auc import *\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "from process_text import text_to_vectors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"logs/predictions.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate cross validation prediction (internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute area under curve and standard deviation using DeLong method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty figure\n",
    "plt.figure(figsize=(7, 5), dpi=450)\n",
    "\n",
    "# Track values over multiple folds\n",
    "aucs = []\n",
    "auc_vars = []\n",
    "\n",
    "# Determine number of folds\n",
    "no_folds = predictions['fold_number'].nunique()\n",
    "\n",
    "# For each fold\n",
    "for i in np.arange(no_folds)+1:\n",
    "    \n",
    "    # Select subset of dataframe corresponding to fold\n",
    "    predictions_fold = predictions[predictions['fold_number'] == i]\n",
    "    \n",
    "    # Compute area under curve and variance based on DeLong method\n",
    "    auc, var = delong_roc_variance(predictions_fold['true_label'], \n",
    "                                   predictions_fold['probability'])\n",
    "\n",
    "    # Track total\n",
    "    aucs.append(auc)\n",
    "    auc_vars.append(var)\n",
    "    \n",
    "    # Compute FPR and TPR rates for plotting\n",
    "    fpr, tpr, thresholds = roc_curve(predictions_fold['true_label'], \n",
    "                                     predictions_fold['probability'])        \n",
    "    \n",
    "    # Add to plot\n",
    "    plt.plot(fpr, tpr, label=\"Fold {} (AUC={:.3f})\".format(i, auc))\n",
    "    \n",
    "# Sampling distribution of the mean \n",
    "auc_mean = np.mean(aucs)\n",
    "auc_var = np.mean(auc_vars)\n",
    "auc_ste = np.sqrt(auc_var) / np.sqrt(no_folds)\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.title('AUROC Site x = {:.3f}'.format(auc_mean))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")    \n",
    "    \n",
    "# Compute area under curve and variance based on DeLong method\n",
    "auc_delong, var_delong = delong_roc_variance(predictions['true_label'], \n",
    "                               predictions['probability'])\n",
    "    \n",
    "print(\"Auc = {:.5f}\".format(auc_mean))\n",
    "print(\"Var = {:.5f}\".format(auc_var))    \n",
    "print(\"Ste = {:.5f}\".format(auc_ste))    \n",
    "print(\"95% CI = {}\".format(stats.norm.ppf([0.025, 0.975],loc=auc_mean, scale=auc_ste)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute binary classification statistics (True Positives, True Negatives, False Positives, False Negatives, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine a binary cutoff for a group of predictions\n",
    "def binary_cutoff(group):\n",
    "    \n",
    "    # Ratio of admissions with a positive outcome in this group\n",
    "    ratio = 1 - (sum(group['true_label']) / len(group['true_label']))\n",
    "    \n",
    "    # Determine threshold based on this ratio\n",
    "    threshold = sorted(group['probability'])[int(ratio * len(group))]\n",
    "    group['binary_prediction'] = (group['probability'] > threshold)\n",
    "    \n",
    "    # Return\n",
    "    return(group)\n",
    "\n",
    "# A binary cutoff is determined for each fold\n",
    "predictions = predictions.groupby(\"fold_number\").apply(binary_cutoff)\n",
    "\n",
    "# Show 2x2 contingency table\n",
    "pd.crosstab(predictions['true_label'], predictions['binary_prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate external models on internal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_external_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if have_external_model:\n",
    "\n",
    "    # Read external paragraph2vec and svm models\n",
    "    p2v_model_external = Doc2Vec.load(\"models_external/paragraph2vec_model\")\n",
    "    svm_model_external = joblib.load(\"models_external/svm_model\")\n",
    "\n",
    "    # Read processed notes\n",
    "    notes = pd.read_csv(\"data/processed/notes.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if have_external_model:\n",
    "    # Obtain vectors of notes using external paragraph2vec model\n",
    "    note_vectors_external = text_to_vectors(notes, 'words_stemmed', p2v_model_external, no_reps=10)\n",
    "\n",
    "    # Predict probabilities using external classification model\n",
    "    probability_external = svm_model_external.predict_proba(note_vectors_external)[:, 1]\n",
    "\n",
    "    # Create dataframe with predictions\n",
    "    predictions_external = pd.DataFrame({'probability' : probability_external, \n",
    "                                     'true_label'  : notes['outcome'].map({0 : False, 1 : True})})\n",
    "\n",
    "    # All predictions in same 'fold'\n",
    "    predictions_external['fold_number'] = 1\n",
    "\n",
    "    # Determine binary cutoff\n",
    "    predictions_external = predictions_external.groupby(\"fold_number\").apply(binary_cutoff)\n",
    "\n",
    "    # Compute area under curve and covariance based on DeLong method\n",
    "    auc_external, auc_var_external = delong_roc_variance(predictions_external['true_label'], \n",
    "                                                 predictions_external['probability'])\n",
    "\n",
    "    print(\"External auc = {:.3f}\".format(auc_external))\n",
    "    print(\"External ste = {:.3f}\".format(np.sqrt(auc_var_external)))\n",
    "    print(\"External 95% CI = {}\".format(stats.norm.ppf([0.025, 0.975],loc=auc_external, scale=np.sqrt(auc_var_external))))\n",
    "\n",
    "    predictions_external.to_csv(\"logs/predictions_external.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THR = 0.06\n",
    "y_pred = [el > PRED_THR for el in predictions.probability.to_list()]\n",
    "y_true = predictions.true_label.to_list()\n",
    "recall = sklearn.metrics.recall_score(y_true, y_pred)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now according to the curve provided above, the false positive rate can be estimated visually to be 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUAL_FPR = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "tn = 0\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i]:\n",
    "        continue\n",
    "    tn += 1\n",
    "    if y_pred[i]:\n",
    "        fp += 1\n",
    "fpr = float(fp) / tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if abs(fpr - VISUAL_FPR) > VISUAL_FPR / 8:\n",
    "    print(\"ERROR: big discrepancy in false-positive rate\")\n",
    "else:\n",
    "    print(\"SUCCESS: false-positive rate is as we expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that if we choose to catch a lot of violence incidents, we will also predict violence incidents incorrectly for a relatively large number of patients. Now remember we have very imbalanced datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_incidents = len([el for el in predictions.true_label.to_list() if el == True])\n",
    "new_incidents = (1 - recall) * original_incidents\n",
    "original_innocent = len([el for el in predictions.true_label.to_list() if el == False])\n",
    "false_incidents = fpr * original_innocent\n",
    "avoided_incidents = recall * original_incidents\n",
    "print(str(int(avoided_incidents)) + \" incidents avoided\")\n",
    "print(str(int(new_incidents)) + \" incidents remain\")\n",
    "print(str(int(false_incidents))  + \" cases incorrectly labeled as aggressive\")\n",
    "print(\"Precision = \" + str(sklearn.metrics.precision_score(y_true, y_pred)))\n",
    "print(\"F1 = \" + str(sklearn.metrics.f1_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the precision is really low, incorrectly classify cases as violent too often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameters chosen by Vincent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_recall = sklearn.metrics.recall_score(predictions.true_label.to_list(), predictions.binary_prediction.to_list())\n",
    "official_fp = len(predictions[(predictions['true_label'] == False) & (predictions['binary_prediction'] == True)])\n",
    "print(str(int(official_recall * original_incidents)) + \" incidents avoided\")\n",
    "print(str(int((1 - official_recall) * original_incidents)) + \" incidents remain\")\n",
    "print(str(int(official_fp))  + \" cases incorrectly labeled as aggressive\")\n",
    "print('Precision', sklearn.metrics.precision_score(predictions.true_label, predictions.binary_prediction))\n",
    "print('F1', sklearn.metrics.f1_score(predictions.true_label, predictions.binary_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would mean less incidents are avoided but also fewer innocents are labelled as aggressive. The threshold should be decided one you know what each entails. For example, if aggression is extremely bad (because people get hurt), then we would go to a model with more false positives. If, on the other hand, aggression only means that more personnel have to be made available to deal with the violent patients, but on the other hand false positives are a problem because we have to keep track of them, then we'd want to keep the false positive rate also somewhat low. So, indeed, we need a metric that keeps both at reasonable levels. That's why maybe the area under the curve is a good metric. What's the maximum F1 score? And what is the threshold?\n",
    "\n",
    "The expert (Floor Scheepers) says it's better to have a high recall (avoid many violence incidents), as I had suspected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(threshold):\n",
    "    y_pred = predictions['probability'].apply(lambda x : x > threshold)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretise thresholds\n",
    "thresholds = np.arange(0, 1, 0.001)\n",
    "y_true = predictions.true_label.to_list()\n",
    "f1s = []\n",
    "for thr in thresholds:\n",
    "    y_pred = get_labels(thr)\n",
    "    f1s.append(sklearn.metrics.f1_score(y_true, y_pred))\n",
    "    \n",
    "max_f1 = 0\n",
    "max_f1_index = -1\n",
    "for i in range(len(f1s)):\n",
    "    if f1s[i] > max_f1:\n",
    "        max_f1_index = i\n",
    "        max_f1 = f1s[i]\n",
    "print('threshold', thresholds[max_f1_index])\n",
    "print('maximum F1',max_f1)\n",
    "print('precision', sklearn.metrics.precision_score(y_true, get_labels(thresholds[max_f1_index])))\n",
    "print('recall', sklearn.metrics.recall_score(y_true, get_labels(thresholds[max_f1_index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best F1 is close to the official value. The baseline precision and recall and accuracy are\n",
    "\n",
    "p = f\n",
    "\n",
    "r = f\n",
    "\n",
    "acc = 2f^2-2f+1\n",
    "\n",
    "where f is the fraction of positives (incidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_baseline = float(original_incidents) / (original_incidents + original_innocent)\n",
    "r_baseline = p_baseline\n",
    "acc_baseline = 2 * p_baseline ** 2 - 2 * p_baseline + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('baseline precision and recall', p_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice conclusion of this is that, even in the high-recall-low-precision scenario that I wrote above, this model performs better than the baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
